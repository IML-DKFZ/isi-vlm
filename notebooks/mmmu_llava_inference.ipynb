{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from llava.data_utils.data_utils import (\n",
    "    load_yaml,\n",
    "    construct_prompt,\n",
    "    save_json,\n",
    "    process_single_sample,\n",
    "    CAT_SHORT2LONG,\n",
    ")\n",
    "from llava.data_utils.model_utils import call_llava_engine_df, llava_image_processor\n",
    "from llava.data_utils.eval_utils import parse_multi_choice_response, parse_open_response\n",
    "from llava.data_utils.set_seed import set_seed\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(42)\n",
    "\n",
    "processor = None\n",
    "call_model_engine = call_llava_engine_df\n",
    "vis_process_func = llava_image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "sub_dataset_list = []\n",
    "for subject in CAT_SHORT2LONG.values():\n",
    "    sub_dataset = load_dataset(\"MMMU/MMMU\", subject, split=\"validation\")\n",
    "    sub_dataset_list.append(sub_dataset)\n",
    "\n",
    "# merge all dataset\n",
    "dataset = concatenate_datasets(sub_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = get_model_name_from_path(\"liuhaotian/llava-v1.5-13b\")\n",
    "tokenizer, model, vis_processors, _ = load_pretrained_model(\n",
    "    \"liuhaotian/llava-v1.5-13b\", None, model_name, load_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_easy_expl = []\n",
    "for i in range(900):\n",
    "    if dataset[i][\"explanation\"] != \"\" and dataset[i][\"topic_difficulty\"] == \"Easy\":\n",
    "        arr_easy_expl.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.data_utils.model_utils import tokenizer_image_token, deal_with_prompt\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "n = 6\n",
    "sample = process_single_sample(arr_easy_expl[n])\n",
    "\n",
    "# sample = construct_prompt(sample, config)\n",
    "if sample[\"image\"]:\n",
    "    sample[\"image\"] = vis_process_func(sample[\"image\"], vis_processors).to(device)\n",
    "\n",
    "prompt = (\n",
    "    sample[\"question\"]\n",
    "    + sample[\"options\"]\n",
    "    + \"Select the correct answer and reason in one short sentence why it is correct.\"\n",
    ")\n",
    "# prompt = \"What is on the image?\"\n",
    "conv = conv_templates[\"vicuna_v1\"].copy()\n",
    "conv.append_message(conv.roles[0], prompt)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "prompt = deal_with_prompt(prompt, model.config.mm_use_im_start_end)\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .cuda()\n",
    ")\n",
    "image = sample[\"image\"]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    images=torch.zeros_like(image).unsqueeze(0).half().cuda(),\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    top_p=None,\n",
    "    num_beams=5,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
